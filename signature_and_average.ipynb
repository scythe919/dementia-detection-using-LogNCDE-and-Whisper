{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61faeb38-9a5c-4277-b5f1-af8726dfd983",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import signatory\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import random\n",
    "import csv\n",
    "\n",
    "features1 = torch.load('/home/sichengyu/text/NCDE/feature_tensor/feature_2020_0.3_whisper_30.pt')\n",
    "features2 = torch.load('/home/sichengyu/text/NCDE/feature_tensor/feature_2020_0.3_test_whisper_30.pt')\n",
    "labels1   = torch.load('/home/sichengyu/text/NCDE/feature_tensor/labels1_2020_0.3_train_whisper_30.pt')\n",
    "labels2   = torch.load('/home/sichengyu/text/NCDE/feature_tensor/labels2_2020_0.3_test_whisper_30.pt')\n",
    "indices_train = torch.load('/home/sichengyu/text/NCDE/feature_tensor/indices_train_2020_0.3_whisper_30.pt')\n",
    "indices_test  = torch.load('/home/sichengyu/text/NCDE/feature_tensor/indices_test_2020_0.3_whisper_30.pt')\n",
    "\n",
    "features1 = features1.numpy()\n",
    "features2 = features2.numpy()\n",
    "labels1   = labels1.numpy()\n",
    "labels2   = labels2.numpy()\n",
    "\n",
    "indices_train = np.array(indices_train)\n",
    "indices_test  = np.array(indices_test)\n",
    "\n",
    "indices_train = torch.from_numpy(indices_train)\n",
    "indices_test  = torch.from_numpy(indices_test)\n",
    "\n",
    "print(\"features1 shape:\", features1.shape)\n",
    "print(\"features2 shape:\", features2.shape)\n",
    "print(\"indices_train shape:\", indices_train.shape)\n",
    "print(\"indices_test shape:\", indices_test.shape)\n",
    "\n",
    "y_train = labels1\n",
    "y_test  = labels2\n",
    "\n",
    "features1_torch = torch.from_numpy(features1).float()  # (N, T, C)\n",
    "features2_torch = torch.from_numpy(features2).float()\n",
    "\n",
    "audio_ids_test = indices_test[:, 0]\n",
    "\n",
    "print(\"Calculating signature features (training set)...\")\n",
    "X_train_sig_torch = signatory.signature(features1_torch, depth=2)\n",
    "print(\"Calculating signature features (test set)...\")\n",
    "X_test_sig_torch  = signatory.signature(features2_torch, depth=2)\n",
    "\n",
    "X_train_sig = X_train_sig_torch.numpy()\n",
    "X_test_sig  = X_test_sig_torch.numpy()\n",
    "\n",
    "print(\"Calculating mean features (training set)...\")\n",
    "X_train_mean_torch = features1_torch.mean(dim=1)\n",
    "print(\"Calculating mean features (test set)...\")\n",
    "X_test_mean_torch  = features2_torch.mean(dim=1)\n",
    "\n",
    "X_train_mean = X_train_mean_torch.numpy()\n",
    "X_test_mean  = X_test_mean_torch.numpy()\n",
    "\n",
    "# These are the labels for the ADReSS dataset, where 0 indicates healthy and 1 indicates dementia. Adjust parameters according to the specific dataset.\n",
    "labels_test = np.concatenate([np.zeros(24), np.ones(24)])  \n",
    "labels_train = np.concatenate([np.zeros(54), np.ones(54)])\n",
    "\n",
    "def run_experiment(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # signature\n",
    "    base_estimator = DecisionTreeClassifier(random_state=seed)\n",
    "    model_sig = BaggingClassifier(\n",
    "        estimator=base_estimator,\n",
    "        n_estimators=100,\n",
    "        random_state=seed,\n",
    "        verbose=0,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model_sig.fit(X_train_sig, y_train)\n",
    "    y_pred_segments_sig = model_sig.predict(X_test_sig)\n",
    "    \n",
    "        \n",
    "    audio_segments_test = {}\n",
    "    for idx, pred_val in zip(indices_test[:, 0], y_pred_segments_sig):\n",
    "        idx = int(idx.item()) if hasattr(idx, 'item') else int(idx)\n",
    "        if idx not in audio_segments_test:\n",
    "            audio_segments_test[idx] = []\n",
    "        audio_segments_test[idx].append(pred_val)\n",
    "    \n",
    "    audio_predictions_test = {}\n",
    "    for idx, preds in audio_segments_test.items():\n",
    "        count_gt_0_5 = sum(1 for pred in preds if pred > 0.5)\n",
    "        count_le_0_5 = len(preds) - count_gt_0_5\n",
    "        final_pred = 1 if count_gt_0_5 > count_le_0_5 else 0\n",
    "        audio_predictions_test[idx] = final_pred\n",
    "    \n",
    "    correct_count = 0\n",
    "    predict_label = []\n",
    "    for idx, avg_pred in audio_predictions_test.items():\n",
    "        true_label = labels_test[idx]\n",
    "        pred_label = 1 if avg_pred > 0.5 else 0\n",
    "        predict_label.append((avg_pred > 0.5))\n",
    "        if pred_label == true_label:\n",
    "            correct_count += 1\n",
    "    \n",
    "    audio_acc_sig = correct_count / len(audio_predictions_test)\n",
    "    from sklearn.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "    \n",
    "    audio_f1_sig  = f1_score(labels_test, predict_label, average='macro')\n",
    "    audio_pre_sig = precision_score(labels_test, predict_label, average='macro')\n",
    "    audio_rec_sig = recall_score(labels_test, predict_label, average='macro')\n",
    "\n",
    "    # Average-pooling\n",
    "    base_estimator2 = DecisionTreeClassifier(random_state=seed)\n",
    "    model_mean = BaggingClassifier(\n",
    "        estimator=base_estimator2,\n",
    "        n_estimators=100,\n",
    "        random_state=seed,\n",
    "        verbose=0,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model_mean.fit(X_train_mean, y_train)\n",
    "    y_pred_segments_mean = model_mean.predict(X_test_mean)\n",
    "\n",
    "    audio_segments_test = {}\n",
    "    for idx, pred_val in zip(indices_test[:, 0], y_pred_segments_mean):\n",
    "        idx = int(idx.item()) if hasattr(idx, 'item') else int(idx)\n",
    "        if idx not in audio_segments_test:\n",
    "            audio_segments_test[idx] = []\n",
    "        audio_segments_test[idx].append(pred_val)\n",
    "    \n",
    "    audio_predictions_test = {}\n",
    "    for idx, preds in audio_segments_test.items():\n",
    "        count_gt_0_5 = sum(1 for pred in preds if pred > 0.5)\n",
    "        count_le_0_5 = len(preds) - count_gt_0_5\n",
    "        final_pred = 1 if count_gt_0_5 > count_le_0_5 else 0\n",
    "        audio_predictions_test[idx] = final_pred\n",
    "        \n",
    "    correct_count = 0\n",
    "    predict_label = []\n",
    "    for idx, avg_pred in audio_predictions_test.items():\n",
    "        true_label = labels_test[idx]\n",
    "        pred_label = 1 if avg_pred > 0.5 else 0\n",
    "        predict_label.append((avg_pred > 0.5))\n",
    "        if pred_label == true_label:\n",
    "            correct_count += 1\n",
    "    \n",
    "    audio_acc_mean = correct_count / len(audio_predictions_test)\n",
    " \n",
    "    audio_f1_mean  = f1_score(labels_test, predict_label, average='macro')\n",
    "    audio_pre_mean = precision_score(labels_test, predict_label, average='macro')\n",
    "    audio_rec_mean = recall_score(labels_test, predict_label, average='macro')\n",
    "\n",
    "    return (audio_acc_sig, audio_f1_sig, audio_pre_sig, audio_rec_sig,\n",
    "            audio_acc_mean, audio_f1_mean, audio_pre_mean, audio_rec_mean)\n",
    "\n",
    "results = []\n",
    "num_seeds = 50\n",
    "all_seeds = seeds = np.arange(1001, 1051).tolist()\n",
    "\n",
    "for seed in all_seeds:\n",
    "    print(f\"\\n===== Starting experiment {seed+1}/{num_seeds} (seed={seed}) =====\")\n",
    "    (acc_sig, f1_sig, pre_sig, rec_sig,\n",
    "     acc_mean, f1_mean, pre_mean, rec_mean) = run_experiment(seed)\n",
    "    \n",
    "    print(f\"[Signature method] Audio level: accuracy={acc_sig:.4f}, f1={f1_sig:.4f}, precision={pre_sig:.4f}, recall={rec_sig:.4f}\")\n",
    "    print(f\"[Mean method] Audio level: accuracy={acc_mean:.4f}, f1={f1_mean:.4f}, precision={pre_mean:.4f}, recall={rec_mean:.4f}\")\n",
    "\n",
    "    # Record results to list\n",
    "    results.append([\n",
    "        seed,\n",
    "        acc_sig, f1_sig, pre_sig, rec_sig,\n",
    "        acc_mean, f1_mean, pre_mean, rec_mean\n",
    "    ])\n",
    "\n",
    "csv_filename = \"solution/signature_experiment_results_2020mean_x.csv\"\n",
    "with open(csv_filename, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\n",
    "        \"seed\",\n",
    "        \"sign_acc\", \"sign_f1\", \"sign_precision\", \"sign_recall\",\n",
    "        \"mean_acc\", \"mean_f1\", \"mean_precision\", \"mean_recall\"\n",
    "    ])\n",
    "    for row in results:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"\\nAll {num_seeds} experiments completed, results saved to {csv_filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85443c1b-1990-4cb1-9f7f-f4a5600bce28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
