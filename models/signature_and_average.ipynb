{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61faeb38-9a5c-4277-b5f1-af8726dfd983",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_572442/3155515684.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  features1 = torch.load('/home/sichengyu/text/NCDE/feature_tensor/feature_2020_0.3_whisper_30.pt')\n",
      "/tmp/ipykernel_572442/3155515684.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  features2 = torch.load('/home/sichengyu/text/NCDE/feature_tensor/feature_2020_0.3_test_whisper_30.pt')\n",
      "/tmp/ipykernel_572442/3155515684.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  labels1   = torch.load('/home/sichengyu/text/NCDE/feature_tensor/labels1_2020_0.3_train_whisper_30.pt')\n",
      "/tmp/ipykernel_572442/3155515684.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  labels2   = torch.load('/home/sichengyu/text/NCDE/feature_tensor/labels2_2020_0.3_test_whisper_30.pt')\n",
      "/tmp/ipykernel_572442/3155515684.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  indices_train = torch.load('/home/sichengyu/text/NCDE/feature_tensor/indices_train_2020_0.3_whisper_30.pt')\n",
      "/tmp/ipykernel_572442/3155515684.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  indices_test  = torch.load('/home/sichengyu/text/NCDE/feature_tensor/indices_test_2020_0.3_whisper_30.pt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features1 shape: (384, 1500, 768)\n",
      "features2 shape: (198, 1500, 768)\n",
      "indices_train shape: torch.Size([384, 2])\n",
      "indices_test shape: torch.Size([198, 2])\n",
      "Calculating signature features (training set)...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m audio_ids_test \u001b[38;5;241m=\u001b[39m indices_test[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalculating signature features (training set)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m X_train_sig_torch \u001b[38;5;241m=\u001b[39m \u001b[43msignatory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures1_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalculating signature features (test set)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     47\u001b[0m X_test_sig_torch  \u001b[38;5;241m=\u001b[39m signatory\u001b[38;5;241m.\u001b[39msignature(features2_torch, depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/Log-NCDE/lib/python3.10/site-packages/signatory/signature_module.py:252\u001b[0m, in \u001b[0;36msignature\u001b[0;34m(path, depth, stream, basepoint, inverse, initial, scalar_term)\u001b[0m\n\u001b[1;32m    250\u001b[0m result \u001b[38;5;241m=\u001b[39m _signature_batch_trick(path, depth, stream, basepoint, inverse, initial, scalar_term)\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# Either because we disabled use of the batch trick, or because the batch trick doesn't apply\u001b[39;00m\n\u001b[0;32m--> 252\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_SignatureFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbasepoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscalar_term\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;66;03m# We have to do the transpose outside of autograd.Function.apply to avoid PyTorch bug 24413\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;66;03m# NOT .transpose_ - the underlying TensorImpl (in C++) is used elsewhere and we don't want to change it.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Log-NCDE/lib/python3.10/site-packages/torch/autograd/function.py:575\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/Log-NCDE/lib/python3.10/site-packages/signatory/signature_module.py:61\u001b[0m, in \u001b[0;36m_SignatureFunction.forward\u001b[0;34m(ctx, path, depth, stream, basepoint, inverse, initial, scalar_term)\u001b[0m\n\u001b[1;32m     57\u001b[0m basepoint, basepoint_value \u001b[38;5;241m=\u001b[39m interpret_basepoint(basepoint, path\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m), path\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), path\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m     58\u001b[0m                                                  path\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     59\u001b[0m initial, initial_value \u001b[38;5;241m=\u001b[39m interpret_initial(initial)\n\u001b[0;32m---> 61\u001b[0m signature_, path_increments \u001b[38;5;241m=\u001b[39m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbasepoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbasepoint_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minverse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m                                                     \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscalar_term\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(signature_, path_increments)\n\u001b[1;32m     64\u001b[0m ctx\u001b[38;5;241m.\u001b[39mdepth \u001b[38;5;241m=\u001b[39m depth\n",
      "File \u001b[0;32m~/anaconda3/envs/Log-NCDE/lib/python3.10/site-packages/signatory/impl.py:36\u001b[0m, in \u001b[0;36m_wrap.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 36\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(e))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import signatory\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import random\n",
    "import csv\n",
    "\n",
    "features1 = torch.load('/home/sichengyu/text/NCDE/feature_tensor/feature_2020_0.3_whisper_30.pt')\n",
    "features2 = torch.load('/home/sichengyu/text/NCDE/feature_tensor/feature_2020_0.3_test_whisper_30.pt')\n",
    "labels1   = torch.load('/home/sichengyu/text/NCDE/feature_tensor/labels1_2020_0.3_train_whisper_30.pt')\n",
    "labels2   = torch.load('/home/sichengyu/text/NCDE/feature_tensor/labels2_2020_0.3_test_whisper_30.pt')\n",
    "indices_train = torch.load('/home/sichengyu/text/NCDE/feature_tensor/indices_train_2020_0.3_whisper_30.pt')\n",
    "indices_test  = torch.load('/home/sichengyu/text/NCDE/feature_tensor/indices_test_2020_0.3_whisper_30.pt')\n",
    "\n",
    "features1 = features1.numpy()\n",
    "features2 = features2.numpy()\n",
    "labels1   = labels1.numpy()\n",
    "labels2   = labels2.numpy()\n",
    "\n",
    "indices_train = np.array(indices_train)\n",
    "indices_test  = np.array(indices_test)\n",
    "\n",
    "indices_train = torch.from_numpy(indices_train)\n",
    "indices_test  = torch.from_numpy(indices_test)\n",
    "\n",
    "print(\"features1 shape:\", features1.shape)\n",
    "print(\"features2 shape:\", features2.shape)\n",
    "print(\"indices_train shape:\", indices_train.shape)\n",
    "print(\"indices_test shape:\", indices_test.shape)\n",
    "\n",
    "y_train = labels1\n",
    "y_test  = labels2\n",
    "\n",
    "features1_torch = torch.from_numpy(features1).float()  # (N, T, C)\n",
    "features2_torch = torch.from_numpy(features2).float()\n",
    "\n",
    "audio_ids_test = indices_test[:, 0]\n",
    "\n",
    "print(\"Calculating signature features (training set)...\")\n",
    "X_train_sig_torch = signatory.signature(features1_torch, depth=2)\n",
    "print(\"Calculating signature features (test set)...\")\n",
    "X_test_sig_torch  = signatory.signature(features2_torch, depth=2)\n",
    "\n",
    "X_train_sig = X_train_sig_torch.numpy()\n",
    "X_test_sig  = X_test_sig_torch.numpy()\n",
    "\n",
    "print(\"Calculating mean features (training set)...\")\n",
    "X_train_mean_torch = features1_torch.mean(dim=1)\n",
    "print(\"Calculating mean features (test set)...\")\n",
    "X_test_mean_torch  = features2_torch.mean(dim=1)\n",
    "\n",
    "X_train_mean = X_train_mean_torch.numpy()\n",
    "X_test_mean  = X_test_mean_torch.numpy()\n",
    "\n",
    "def run_experiment(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # signature\n",
    "    base_estimator = DecisionTreeClassifier(random_state=seed)\n",
    "    model_sig = BaggingClassifier(\n",
    "        estimator=base_estimator,\n",
    "        n_estimators=100,\n",
    "        random_state=seed,\n",
    "        verbose=0,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model_sig.fit(X_train_sig, y_train)\n",
    "    y_pred_segments_sig = model_sig.predict(X_test_sig)\n",
    "    \n",
    "    # These are the labels for the ADReSS dataset, where 0 indicates healthy and 1 indicates dementia. Adjust parameters according to the specific dataset.\n",
    "    labels_test = np.concatenate([np.zeros(24), np.ones(24)])  \n",
    "    labels_train = np.concatenate([np.zeros(54), np.ones(54)])\n",
    "        \n",
    "    audio_segments_test = {}\n",
    "    for idx, pred_val in zip(indices_test[:, 0], y_pred_segments_sig):\n",
    "        idx = int(idx.item()) if hasattr(idx, 'item') else int(idx)\n",
    "        if idx not in audio_segments_test:\n",
    "            audio_segments_test[idx] = []\n",
    "        audio_segments_test[idx].append(pred_val)\n",
    "    \n",
    "    audio_predictions_test = {}\n",
    "    for idx, preds in audio_segments_test.items():\n",
    "        count_gt_0_5 = sum(1 for pred in preds if pred > 0.5)\n",
    "        count_le_0_5 = len(preds) - count_gt_0_5\n",
    "        final_pred = 1 if count_gt_0_5 > count_le_0_5 else 0\n",
    "        audio_predictions_test[idx] = final_pred\n",
    "    \n",
    "    correct_count = 0\n",
    "    predict_label = []\n",
    "    for idx, avg_pred in audio_predictions_test.items():\n",
    "        true_label = labels_test[idx]\n",
    "        pred_label = 1 if avg_pred > 0.5 else 0\n",
    "        predict_label.append((avg_pred > 0.5))\n",
    "        if pred_label == true_label:\n",
    "            correct_count += 1\n",
    "    \n",
    "    audio_acc_sig = correct_count / len(audio_predictions_test)\n",
    "    from sklearn.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "    \n",
    "    audio_f1_sig  = f1_score(labels_test, predict_label, average='macro')\n",
    "    audio_pre_sig = precision_score(labels_test, predict_label, average='macro')\n",
    "    audio_rec_sig = recall_score(labels_test, predict_label, average='macro')\n",
    "\n",
    "    # Average-pooling\n",
    "    base_estimator2 = DecisionTreeClassifier(random_state=seed)\n",
    "    model_mean = BaggingClassifier(\n",
    "        estimator=base_estimator2,\n",
    "        n_estimators=100,\n",
    "        random_state=seed,\n",
    "        verbose=0,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model_mean.fit(X_train_mean, y_train)\n",
    "    y_pred_segments_mean = model_mean.predict(X_test_mean)\n",
    "\n",
    "    audio_segments_test = {}\n",
    "    for idx, pred_val in zip(indices_test[:, 0], y_pred_segments_mean):\n",
    "        idx = int(idx.item()) if hasattr(idx, 'item') else int(idx)\n",
    "        if idx not in audio_segments_test:\n",
    "            audio_segments_test[idx] = []\n",
    "        audio_segments_test[idx].append(pred_val)\n",
    "    \n",
    "    audio_predictions_test = {}\n",
    "    for idx, preds in audio_segments_test.items():\n",
    "        count_gt_0_5 = sum(1 for pred in preds if pred > 0.5)\n",
    "        count_le_0_5 = len(preds) - count_gt_0_5\n",
    "        final_pred = 1 if count_gt_0_5 > count_le_0_5 else 0\n",
    "        audio_predictions_test[idx] = final_pred\n",
    "        \n",
    "    correct_count = 0\n",
    "    predict_label = []\n",
    "    for idx, avg_pred in audio_predictions_test.items():\n",
    "        true_label = labels_test[idx]\n",
    "        pred_label = 1 if avg_pred > 0.5 else 0\n",
    "        predict_label.append((avg_pred > 0.5))\n",
    "        if pred_label == true_label:\n",
    "            correct_count += 1\n",
    "    \n",
    "    audio_acc_mean = correct_count / len(audio_predictions_test)\n",
    " \n",
    "    audio_f1_mean  = f1_score(labels_test, predict_label, average='macro')\n",
    "    audio_pre_mean = precision_score(labels_test, predict_label, average='macro')\n",
    "    audio_rec_mean = recall_score(labels_test, predict_label, average='macro')\n",
    "\n",
    "    return (audio_acc_sig, audio_f1_sig, audio_pre_sig, audio_rec_sig,\n",
    "            audio_acc_mean, audio_f1_mean, audio_pre_mean, audio_rec_mean)\n",
    "\n",
    "results = []\n",
    "num_seeds = 50\n",
    "all_seeds = seeds = np.arange(1001, 1051).tolist()\n",
    "\n",
    "for seed in all_seeds:\n",
    "    print(f\"\\n===== Starting experiment {seed+1}/{num_seeds} (seed={seed}) =====\")\n",
    "    (acc_sig, f1_sig, pre_sig, rec_sig,\n",
    "     acc_mean, f1_mean, pre_mean, rec_mean) = run_experiment(seed)\n",
    "    \n",
    "    print(f\"[Signature method] Audio level: accuracy={acc_sig:.4f}, f1={f1_sig:.4f}, precision={pre_sig:.4f}, recall={rec_sig:.4f}\")\n",
    "    print(f\"[Mean method] Audio level: accuracy={acc_mean:.4f}, f1={f1_mean:.4f}, precision={pre_mean:.4f}, recall={rec_mean:.4f}\")\n",
    "\n",
    "    # Record results to list\n",
    "    results.append([\n",
    "        seed,\n",
    "        acc_sig, f1_sig, pre_sig, rec_sig,\n",
    "        acc_mean, f1_mean, pre_mean, rec_mean\n",
    "    ])\n",
    "\n",
    "csv_filename = \"solution/signature_experiment_results_2020mean_x.csv\"\n",
    "with open(csv_filename, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\n",
    "        \"seed\",\n",
    "        \"sign_acc\", \"sign_f1\", \"sign_precision\", \"sign_recall\",\n",
    "        \"mean_acc\", \"mean_f1\", \"mean_precision\", \"mean_recall\"\n",
    "    ])\n",
    "    for row in results:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"\\nAll {num_seeds} experiments completed, results saved to {csv_filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85443c1b-1990-4cb1-9f7f-f4a5600bce28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
