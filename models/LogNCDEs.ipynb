{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b246d8-a5e3-4092-8578-eabde8f2990f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.95\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bfa26e-5307-4c37-ad38-c9ce12b72683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import itertools\n",
    "import math\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from typing import Generator, Union\n",
    "\n",
    "import diffrax\n",
    "import equinox as eqx\n",
    "import IPython\n",
    "import jax\n",
    "import jax.nn as jnn\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import jax.scipy as jsp\n",
    "import librosa\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optax\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from jax import nn as jnn\n",
    "from scipy.sparse import csc_matrix\n",
    "# Import specific functions from signax for handling log-signature calculation\n",
    "from signax.signature import signature  # For computing signatures of paths\n",
    "from signax.signature_flattened import flatten  # For flattening signatures\n",
    "from signax.tensor_ops import log  # For converting to log-signatures\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from torchaudio.utils import download_asset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3f1b12-3fdd-41b0-8ef8-2a9b439c4d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tkey_to_index(width: int, tkey: Union[int, tuple[int]]) -> int:\n",
    "    if isinstance(tkey, int):\n",
    "        assert tkey <= width\n",
    "        return tkey\n",
    "\n",
    "    result = 0\n",
    "    for letter in tkey:\n",
    "        result *= width\n",
    "        result += letter\n",
    "    return result\n",
    "\n",
    "\n",
    "def tensor_algebra_dimension(width: int, depth: int) -> int:\n",
    "    result = 1\n",
    "    for _ in range(depth):\n",
    "        result *= width\n",
    "        result += 1\n",
    "    return result\n",
    "\n",
    "\n",
    "def generate_tensor_keys_level(\n",
    "    width: int, degree: int\n",
    ") -> Generator[tuple[int], None, None]:\n",
    "    if degree == 1:\n",
    "        yield from ((i,) for i in range(1, width + 1))\n",
    "        return\n",
    "\n",
    "    for i in range(1, width + 1):\n",
    "        yield from ((i, *r) for r in generate_tensor_keys_level(width, degree - 1))\n",
    "\n",
    "\n",
    "def generate_tensor_keys(width: int, depth: int) -> Generator[tuple[int], None, None]:\n",
    "    yield ()\n",
    "\n",
    "    if depth == 0:\n",
    "        return\n",
    "\n",
    "    for i in range(1, width + 1):\n",
    "        yield (i,)\n",
    "\n",
    "    for degree in range(2, depth + 1):\n",
    "        yield from generate_tensor_keys_level(width, degree)\n",
    "\n",
    "\n",
    "class HallSet:\n",
    "    def __init__(self, width, degree=1):\n",
    "        self.width = width\n",
    "        self.degree = 1\n",
    "\n",
    "        self.data = data = []\n",
    "        self.reverse_map = reverse_map = {}\n",
    "        self.degree_ranges = degree_ranges = []\n",
    "        self.sizes = sizes = []\n",
    "        self.letters = letters = []\n",
    "        self.l2k = l2k = {}\n",
    "\n",
    "        data.append((0, 0))\n",
    "        degree_ranges.append((0, 1))\n",
    "        sizes.append(0)\n",
    "\n",
    "        for letter in range(1, width + 1):\n",
    "            parents = (0, letter)\n",
    "            letters.append(letter)\n",
    "            data.append(parents)\n",
    "            reverse_map[parents] = letter\n",
    "            l2k[letter] = letter\n",
    "\n",
    "        degree_ranges.append((degree_ranges[0][1], len(data)))\n",
    "        sizes.append(width)\n",
    "\n",
    "        if degree > self.degree:\n",
    "            self.grow_up(degree)\n",
    "\n",
    "    def grow_up(self, degree):\n",
    "\n",
    "        data = self.data\n",
    "        reverse_map = self.reverse_map\n",
    "        degree_ranges = self.degree_ranges\n",
    "\n",
    "        while self.degree < degree:\n",
    "            next_degree = self.degree + 1\n",
    "            left = 1\n",
    "            while 2 * left <= next_degree:\n",
    "                right = next_degree - left\n",
    "\n",
    "                ilower, iupper = degree_ranges[left]\n",
    "                jlower, jupper = degree_ranges[right]\n",
    "\n",
    "                i = ilower\n",
    "\n",
    "                while i < iupper:\n",
    "                    j = max(jlower, i + 1)\n",
    "                    while j < jupper:\n",
    "                        if data[j][0] <= i:\n",
    "                            parents = (i, j)\n",
    "                            data.append(parents)\n",
    "                            reverse_map[parents] = len(data) - 1\n",
    "                        j += 1\n",
    "                    i += 1\n",
    "                left += 1\n",
    "\n",
    "            degree_ranges.append((degree_ranges[-1][1], len(data)))\n",
    "            self.sizes.append(len(data))\n",
    "            self.degree += 1\n",
    "\n",
    "    @functools.lru_cache\n",
    "    def key_to_string(self, key: int) -> str:\n",
    "        assert key < len(self.data)\n",
    "\n",
    "        left, right = self.data[key]\n",
    "\n",
    "        if left == 0:\n",
    "            return f\"{right}\"\n",
    "\n",
    "        return f\"[{self.key_to_string(left)}, {self.key_to_string(right)}]\"\n",
    "\n",
    "    @functools.lru_cache\n",
    "    def product(self, lhs_key: int, rhs_key: int) -> list[tuple[int, int]]:\n",
    "        if rhs_key < lhs_key:\n",
    "            return [(k, -c) for k, c in self.product(rhs_key, lhs_key)]\n",
    "\n",
    "        if lhs_key == rhs_key:\n",
    "            return []\n",
    "\n",
    "        if key := self.reverse_map.get((lhs_key, rhs_key)):\n",
    "            return [(key, 1)]\n",
    "\n",
    "        lparent, rparent = self.data[rhs_key]\n",
    "\n",
    "        left_result = [\n",
    "            (k, c1 * c)\n",
    "            for (k1, c1) in self.product(lhs_key, lparent)\n",
    "            for (k, c) in self.product(k1, rparent)\n",
    "        ]\n",
    "        right_result = [\n",
    "            (k, -c1 * c)\n",
    "            for (k1, c1) in self.product(lhs_key, rparent)\n",
    "            for (k, c) in self.product(k1, lparent)\n",
    "        ]\n",
    "        result = defaultdict(lambda: 0)\n",
    "        for k, c in left_result:\n",
    "            result[k] += c\n",
    "        for k, c in right_result:\n",
    "            result[k] += c\n",
    "\n",
    "        return list(result.items())\n",
    "\n",
    "    @functools.lru_cache\n",
    "    def expand(self, key: int) -> list[tuple[int, tuple[int]]]:\n",
    "        if key in self.letters:\n",
    "            return [((key,), 1)]\n",
    "\n",
    "        assert key < len(self.data)\n",
    "        lparent, rparent = self.data[key]\n",
    "\n",
    "        left_expansion = self.expand(lparent)\n",
    "        right_expansion = self.expand(rparent)\n",
    "\n",
    "        left_terms = [\n",
    "            ((*k1, *k2), c1 * c2)\n",
    "            for (k1, c1), (k2, c2) in itertools.product(left_expansion, right_expansion)\n",
    "        ]\n",
    "        right_terms = [\n",
    "            ((*k1, *k2), c1 * c2)\n",
    "            for (k1, c1), (k2, c2) in itertools.product(right_expansion, left_expansion)\n",
    "        ]\n",
    "\n",
    "        result = defaultdict(lambda: 0)\n",
    "        for k, c in left_terms:\n",
    "            result[k] += c\n",
    "        for k, c in right_terms:\n",
    "            result[k] -= c\n",
    "\n",
    "        return list(result.items())\n",
    "\n",
    "    @functools.lru_cache\n",
    "    def rbracket(self, tkey: Union[int, tuple[int]]) -> list[tuple[int, int]]:\n",
    "        if isinstance(tkey, int):\n",
    "            return [(tkey, 1)]\n",
    "\n",
    "        if len(tkey) == 0:\n",
    "            return []\n",
    "\n",
    "        if len(tkey) == 1:\n",
    "            return [(tkey[0], 1)]\n",
    "\n",
    "        assert len(tkey) > 1, f\"{tkey}\"\n",
    "        first, *remaining = tkey\n",
    "        return [\n",
    "            (k, c1 * c)\n",
    "            for (k1, c1) in self.rbracket(tuple(remaining))\n",
    "            for k, c in self.product(first, k1)\n",
    "        ]\n",
    "\n",
    "    def l2t_matrix(self, degree=None, dtype=np.float32) -> jnp.ndarray:\n",
    "        degree = degree or self.degree\n",
    "        tensor_alg_size = tensor_algebra_dimension(self.width, degree)\n",
    "\n",
    "        indptr = [0, 0]\n",
    "        indices = []\n",
    "        data = []\n",
    "        for lkey in range(1, self.sizes[degree]):\n",
    "            for k, c in self.expand(lkey):\n",
    "                indices.append(tkey_to_index(self.width, k))\n",
    "                data.append(c)\n",
    "            indptr.append(indptr[-1] + len(self.expand(lkey)))\n",
    "\n",
    "        data = np.array(data, dtype=dtype)\n",
    "        indices = np.array(indices, dtype=np.int64)\n",
    "        indptr = np.array(indptr, dtype=np.int64)\n",
    "        return jnp.array(\n",
    "            csc_matrix(\n",
    "                (data, indices, indptr),\n",
    "                shape=(tensor_alg_size, self.sizes[degree]),\n",
    "                dtype=dtype,\n",
    "            ).toarray()\n",
    "        )\n",
    "\n",
    "    def t2l_matrix(self, degree=None, dtype=np.float32) -> jnp.ndarray:\n",
    "        degree = degree or self.degree\n",
    "        tensor_alg_size = tensor_algebra_dimension(self.width, degree)\n",
    "\n",
    "        indptr = [0]\n",
    "        indices = []\n",
    "        data = []\n",
    "        for tkey in generate_tensor_keys(self.width, degree):\n",
    "            for k, c in self.rbracket(tkey):\n",
    "                indices.append(k)\n",
    "                data.append(c / len(tkey))\n",
    "            indptr.append(len(data))\n",
    "        data = np.array(data, dtype=dtype)\n",
    "        indices = np.array(indices, dtype=np.int64)\n",
    "        indptr = np.array(indptr, dtype=np.int64)\n",
    "\n",
    "        return jnp.array(\n",
    "            csc_matrix(\n",
    "                (data, indices, indptr),\n",
    "                shape=(self.sizes[degree], tensor_alg_size),\n",
    "                dtype=dtype,\n",
    "            ).toarray()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0508af-1092-40f9-a92f-197e7ef473a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remat_conv(conv_layer, x):\n",
    "    return jax.remat(conv_layer)(x)\n",
    "class VectorField(eqx.Module):\n",
    "    mlp: eqx.nn.MLP\n",
    "\n",
    "    def __init__(\n",
    "        self, in_size, out_size, width, depth, *, key,scale=1000\n",
    "    ):\n",
    "        mlp = eqx.nn.MLP(\n",
    "            in_size=in_size,\n",
    "            out_size=out_size,\n",
    "            width_size=width,\n",
    "            depth=depth,\n",
    "            activation=jax.nn.silu,\n",
    "            final_activation=jax.nn.tanh,\n",
    "            key=key,\n",
    "        )\n",
    "\n",
    "        def init_weight(model):\n",
    "            is_linear = lambda x: isinstance(x, eqx.nn.Linear)\n",
    "            get_weights = lambda m: [\n",
    "                x.weight\n",
    "                for x in jax.tree_util.tree_leaves(m, is_leaf=is_linear)\n",
    "                if is_linear(x)\n",
    "            ]\n",
    "            weights = get_weights(model)\n",
    "            new_weights = [weight / scale for weight in weights]\n",
    "            new_model = eqx.tree_at(get_weights, model, new_weights)\n",
    "            get_bias = lambda m: [\n",
    "                x.bias\n",
    "                for x in jax.tree_util.tree_leaves(m, is_leaf=is_linear)\n",
    "                if is_linear(x)\n",
    "            ]\n",
    "            biases = get_bias(model)\n",
    "            new_bias = [bias / scale for bias in biases]\n",
    "            new_model = eqx.tree_at(get_bias, new_model, new_bias)\n",
    "            return new_model\n",
    "\n",
    "        self.mlp = init_weight(mlp)\n",
    "\n",
    "    def __call__(self, y):\n",
    "        return self.mlp(y)\n",
    "        \n",
    "# Define the NeuralCDE class\n",
    "class NeuralCDE(eqx.Module):\n",
    "    vf: eqx.nn.MLP  \n",
    "    data_dim: int \n",
    "    hidden_dim: int \n",
    "    ode_solver_stepsize: int \n",
    "    linear1: eqx.nn.Linear \n",
    "    linear2: eqx.nn.Linear  \n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim,\n",
    "        data_dim,\n",
    "        label_dim,\n",
    "        vf_hidden_dim,\n",
    "        vf_num_hidden,\n",
    "        ode_solver_stepsize,\n",
    "        *,\n",
    "        key,\n",
    "    ):\n",
    "\n",
    "        vf_key, l1key, l2key, conv_key = jr.split(key, 4)\n",
    "                \n",
    "        # Initialise the MLP vector field\n",
    "        self.vf = VectorField(\n",
    "            hidden_dim,\n",
    "            hidden_dim * data_dim,\n",
    "            vf_hidden_dim,\n",
    "            vf_num_hidden,\n",
    "            scale=1,\n",
    "            key=vf_key,\n",
    "        )\n",
    "\n",
    "        self.linear1 = eqx.nn.Linear(data_dim, hidden_dim, key=l1key)\n",
    "        self.linear2 = eqx.nn.Linear(hidden_dim, label_dim, key=l2key)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.data_dim = data_dim\n",
    "        self.ode_solver_stepsize = ode_solver_stepsize\n",
    "        \n",
    "    # Method to get the ODE term\n",
    "    def get_ode(self, ts, X):\n",
    "\n",
    "        # Compute backward Hermite coefficients for interpolation\n",
    "        coeffs = diffrax.backward_hermite_coefficients(ts, X)\n",
    "        # Create a cubic interpolation control term\n",
    "        control = diffrax.CubicInterpolation(ts, coeffs)\n",
    "        # Reshape output from vector field to be matrix-valued\n",
    "        func = lambda t, y, args: jnp.reshape(\n",
    "            self.vf(y), (self.hidden_dim, self.data_dim)\n",
    "        )\n",
    "        # Return the control term converted to an ODE\n",
    "        return diffrax.ControlTerm(func, control).to_ode(), control\n",
    "    \n",
    "    # Forward pass method\n",
    "    def __call__(self, X,key,*,inference=False):\n",
    "        key, dropout_key1, dropout_key2,dropout_key3 = jr.split(key, 4)\n",
    "        ts = X[:, 0] # Assume time is the first channel\n",
    "    \n",
    "        # Separate time and features\n",
    "        X_features = X[:, 1:]  # Features without time\n",
    "        \n",
    "\n",
    "        X_with_time = jnp.concatenate([ts[:, None], X_features], axis=1)\n",
    "        \n",
    "    \n",
    "        # Get the ODE term\n",
    "        result = self.get_ode(ts, X_with_time)\n",
    "    \n",
    "        # Check if get_ode returned control\n",
    "        if isinstance(result, tuple):\n",
    "            ode_term, control = result\n",
    "            # Initialise the hidden state using control\n",
    "            h0 = self.linear1(control.evaluate(ts[0]))\n",
    "        else:\n",
    "            ode_term = result\n",
    "            h0 = self.linear1(X_with_time[0, :])\n",
    "    \n",
    "        saveat = diffrax.SaveAt(t1=True) \n",
    "        # Solve the differential equation\n",
    "        solution = jax.remat(diffrax.diffeqsolve(\n",
    "            terms=ode_term,\n",
    "            solver=diffrax.Heun(),\n",
    "            t0=ts[0],\n",
    "            t1=ts[-1],\n",
    "            dt0=self.ode_solver_stepsize,\n",
    "            y0=h0,\n",
    "            saveat=saveat,\n",
    "            stepsize_controller=diffrax.ConstantStepSize(),\n",
    "        ))\n",
    "        \n",
    "        (prediction,) = jnn.sigmoid(self.linear2(solution.ys[-1]))\n",
    "\n",
    "        return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926b35f6-2d83-4430-9ef4-9407e71a890f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LogNeuralCDE class, which is identical to NeuralCDE, except for the get_ode method\n",
    "class LogNeuralCDE(NeuralCDE):\n",
    "    stepsize: int \n",
    "    depth: int\n",
    "    hall_set: HallSet\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim,\n",
    "        data_dim,\n",
    "        label_dim,\n",
    "        vf_hidden_dim,\n",
    "        vf_num_hidden,\n",
    "        ode_solver_stepsize,\n",
    "        stepsize,\n",
    "        depth,\n",
    "        *,\n",
    "        key,\n",
    "    ):\n",
    "        # Initialise the parent class NeuralCDE\n",
    "        super().__init__(\n",
    "            hidden_dim,\n",
    "            data_dim,\n",
    "            label_dim,\n",
    "            vf_hidden_dim,\n",
    "            vf_num_hidden,\n",
    "            ode_solver_stepsize,\n",
    "            key=key,\n",
    "        )\n",
    "        self.stepsize = stepsize\n",
    "        # Ensure the depth is either 1 or 2\n",
    "        if depth not in [1, 2]:\n",
    "            raise ValueError(\n",
    "                \"The Log-ODE method is only implemented for truncation depths one and two\"\n",
    "            )\n",
    "        self.depth = depth\n",
    "        self.hall_set = HallSet(data_dim, depth)\n",
    "\n",
    "    # Method to calculate log-signatures\n",
    "    def calc_logsigs(self, X):\n",
    "        # Reshape data\n",
    "        X = X.reshape(-1, self.stepsize, X.shape[-1])\n",
    "\n",
    "        # Prepend zero to the first interval and the last element of the previous interval to every other interval\n",
    "        prepend = jnp.concatenate((jnp.zeros((1, X.shape[-1])), X[:-1, -1, :]))[\n",
    "            :, None, :\n",
    "        ]\n",
    "        X = jnp.concatenate((prepend, X), axis=1)\n",
    "\n",
    "        # Define log-signature function\n",
    "        def logsig(x):\n",
    "            logsig = flatten(log(signature(x, self.depth)))\n",
    "            if self.depth == 1:\n",
    "                return jnp.concatenate((jnp.array([0]), logsig))\n",
    "            else:\n",
    "                tensor_to_lie_map = self.hall_set.t2l_matrix(self.depth)\n",
    "                return tensor_to_lie_map[:, 1:] @ logsig\n",
    "\n",
    "        # Calculate log-signatures over each interval\n",
    "        logsigs = jax.vmap(logsig)(X)\n",
    "\n",
    "        return logsigs\n",
    "\n",
    "    # ODE for depth one Log-ODE method\n",
    "    def depth_one_ode(self, y, logsig, interval_length):\n",
    "        vf_out = jnp.reshape(self.vf(y), (self.hidden_dim, self.data_dim))\n",
    "        return jnp.dot(vf_out, logsig[1:]) / interval_length\n",
    "\n",
    "    # ODE for depth two Log-ODE method\n",
    "    def depth_two_ode(self, y, logsig, interval_length):\n",
    "        # Reshape output from vector field to be matrix-valued\n",
    "        vf_out = jnp.reshape(self.vf(y), (self.hidden_dim, self.data_dim))\n",
    "\n",
    "        # Calculate Jacobian-vector products used to compute the Lie brackets\n",
    "        jvps = jnp.reshape(\n",
    "            jax.vmap(lambda x: jax.jvp(self.vf, (y,), (x,))[1])(vf_out.T),\n",
    "            (self.data_dim, self.data_dim, self.hidden_dim),\n",
    "        )\n",
    "\n",
    "        # Compute Lie brackets\n",
    "        def liebracket(jvps, pair):\n",
    "            return jvps[pair[0] - 1, pair[1] - 1] - jvps[pair[1] - 1, pair[0] - 1]\n",
    "\n",
    "        pairs = jnp.asarray(self.hall_set.data[self.data_dim + 1 :])\n",
    "        lieout = jax.vmap(liebracket, in_axes=(None, 0))(jvps, pairs)\n",
    "\n",
    "        # Combine Lie brackets with the log-signature\n",
    "        vf_depth1 = jnp.dot(vf_out, logsig[1 : self.data_dim + 1])\n",
    "        vf_depth2 = jnp.dot(lieout.T, logsig[self.data_dim + 1 :])\n",
    "\n",
    "        return (vf_depth1 + vf_depth2) / interval_length\n",
    "\n",
    "    # Define get_ode using the Log-ODE method\n",
    "    def get_ode(self, ts, X):\n",
    "        # Calculate the log-signatures\n",
    "        logsigs = self.calc_logsigs(X)\n",
    "        # Calculate intervals, assuming 0<=t<=1\n",
    "        intervals = (\n",
    "            jnp.arange(0, X.shape[0] + self.stepsize, self.stepsize) / X.shape[0]\n",
    "        )\n",
    "\n",
    "        # Define ODE function\n",
    "        def func(t, y, args):\n",
    "            idx = jnp.searchsorted(intervals, t)\n",
    "            logsig_t = logsigs[idx - 1]\n",
    "            interval_length = intervals[idx] - intervals[idx - 1]\n",
    "            if self.depth == 1:\n",
    "                return self.depth_one_ode(y, logsig_t, interval_length)\n",
    "            if self.depth == 2:\n",
    "                return self.depth_two_ode(y, logsig_t, interval_length)\n",
    "\n",
    "        return diffrax.ODETerm(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6d7a25-9c51-43ad-bf84-b3f1e9e06059",
   "metadata": {},
   "outputs": [],
   "source": [
    "features1 = np.load('/home/sichengyu/text/NCDE/SimplifiedProgram/autoencode/Pitt/train_features_Pitt_conv_1.npy')\n",
    "features2 = np.load('/home/sichengyu/text/NCDE/SimplifiedProgram/autoencode/Pitt/test_features_Pitt_conv_1.npy')\n",
    "indices_train = torch.load('/home/sichengyu/text/NCDE/feature_tensor/Pittnew/indices_train_Pitt_0.3_whisper_30_new_norm.pt')\n",
    "indices_test = torch.load('/home/sichengyu/text/NCDE/feature_tensor/Pittnew/indices_test_Pitt_0.3_whisper_30_new_norm.pt')\n",
    "labels1=torch.load('/home/sichengyu/text/NCDE/feature_tensor/Pittnew/labels1_Pitt_0.3_train_whisper_30_new_norm.pt')\n",
    "labels2=torch.load('/home/sichengyu/text/NCDE/feature_tensor/Pittnew/labels2_Pitt_0.3_test_whisper_30_new_norm.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b99e98-2353-4a3b-a64f-14f2600417d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_np = features1\n",
    "features_np_test =features2\n",
    "labels1_np=labels1.detach().cpu().numpy()\n",
    "labels2_np=labels2.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "features_jax = jnp.array(features_np)\n",
    "features_jax_test=jnp.array(features_np_test)\n",
    "labels_jax=jnp.array(labels1_np)\n",
    "labels_jax_test=jnp.array(labels2_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a542897-04cb-4e6a-89c7-239e1f685985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(features):\n",
    "    mean = features.mean((0, 1), keepdims=True)  \n",
    "    std = features.std((0, 1), keepdims=True)    \n",
    "    standardized_features = (features - mean) / (std + 1e-8) \n",
    "    return standardized_features\n",
    "\n",
    "def get_data(features):\n",
    "    ts = jnp.linspace(0,1, features.shape[1])  \n",
    "    ts1 = jnp.repeat(ts[None, :], features.shape[0], axis=0)\n",
    "    normalized_features = preprocess_data(features)\n",
    "    time_steps_expanded = ts1[:, :, None]  \n",
    "    features_with_time = jnp.concatenate([time_steps_expanded,normalized_features], axis=2)  \n",
    "\n",
    "    return features_with_time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8340050d-9516-4b9e-baf6-c0b9784989ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=get_data(features_jax)\n",
    "X_test=get_data(features_jax_test)\n",
    "\n",
    "\n",
    "y_train=labels_jax\n",
    "y_test=labels_jax_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2024e9-0d84-410a-8367-aafe111c8603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_audio_files(directory):\n",
    "    audio_extensions = ('.wav', '.mp3', '.flac', '.aac', '.ogg', '.m4a', '.wma')\n",
    "    audio_file_count = 0\n",
    "\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(audio_extensions):\n",
    "                audio_file_count += 1\n",
    "\n",
    "    return audio_file_count\n",
    "\n",
    "train_ccn=count_audio_files(\"/home/sichengyu/Downloads/dementiabank/Pitt_new/norm/train_test_split/train/cc_enhence\")\n",
    "train_cdn=count_audio_files(\"/home/sichengyu/Downloads/dementiabank/Pitt_new/norm/train_test_split/train/cd_enhence\")\n",
    "test_ccn=count_audio_files(\"/home/sichengyu/Downloads/dementiabank/Pitt_new/norm/train_test_split/test/cc_enhence\")\n",
    "test_cdn=count_audio_files(\"/home/sichengyu/Downloads/dementiabank/Pitt_new/norm/train_test_split/test/cd_enhence\")\n",
    "print(\"train_ccn:\",train_ccn)\n",
    "print(\"train_cdn:\",train_cdn)\n",
    "print(\"test_ccn:\",test_ccn)\n",
    "print(\"test_cdn:\",test_cdn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe3b24f-bef4-4f88-9988-49f17f3cb72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_test = jnp.concatenate([jnp.zeros(test_ccn), jnp.ones(test_cdn)])\n",
    "labels_train = jnp.concatenate([jnp.zeros(train_ccn), jnp.ones(train_cdn)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666fa4a8-3253-4577-aab5-1927e8ef9ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994c287a-2d39-414c-beb0-d5f76f49c9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataloader:\n",
    "    data: jnp.ndarray \n",
    "    labels: jnp.ndarray  \n",
    "    size: int \n",
    "\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data  \n",
    "        self.labels = labels \n",
    "        self.size = len(data) \n",
    "\n",
    "    def loop(self, batch_size, *, key=None):\n",
    "        if batch_size == self.size:\n",
    "            yield self.data, self.labels\n",
    "\n",
    "        indices = jnp.arange(self.size)  \n",
    "        while True:\n",
    "            subkey, key = jr.split(key)  \n",
    "            perm = jr.permutation(subkey, indices)  \n",
    "            start = 0\n",
    "            end = batch_size\n",
    "            while end < self.size:\n",
    "                batch_perm = perm[start:end] \n",
    "                yield self.data[batch_perm], self.labels[batch_perm]\n",
    "                start = end  \n",
    "                end = start + batch_size  \n",
    "\n",
    "\n",
    "# Initialise dataloaders for training and testing data\n",
    "train_dataloader = Dataloader(X_train, y_train)\n",
    "test_dataloader = Dataloader(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7404be35-c1f1-4ccb-bdc6-0d4974020aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classification loss function with gradient calculation\n",
    "@eqx.filter_jit\n",
    "@eqx.filter_value_and_grad\n",
    "def classification_loss(model, X, y, *, key):\n",
    "    batch_size = X.shape[0]\n",
    "\n",
    "    keys = jax.random.split(key, batch_size)\n",
    "    def model_forward(x, k):\n",
    "        return model(x, k, inference=False)\n",
    "\n",
    "\n",
    "    pred_y = jax.vmap(model_forward)(X, keys)\n",
    "    epsilon = 1e-7\n",
    "    pred_y_clipped = jnp.clip(pred_y, epsilon, 1 - epsilon)\n",
    "    loss = - ( y * jnp.log(pred_y_clipped) +  (1 - y) * jnp.log(1 - pred_y_clipped))    \n",
    "    norm = 0\n",
    "    for layer in model.vf.mlp.layers:\n",
    "        norm += jnp.mean(\n",
    "            jnp.linalg.norm(layer.weight, axis=-1)\n",
    "            + jnp.linalg.norm(layer.bias, axis=-1)\n",
    "        )\n",
    "    norm *= 0\n",
    "    return jnp.mean(loss)+norm\n",
    "\n",
    "# Define the training step function with JIT compilation\n",
    "@eqx.filter_jit\n",
    "def train_step(model, X, y, opt, opt_state, *, key):\n",
    "    key, subkey = jr.split(key)\n",
    "    loss, grads = classification_loss(model, X, y,key=subkey)\n",
    "    updates, opt_state = opt.update(grads, opt_state, params=trainable_params)\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return model, opt_state, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c2a847-9c32-499f-b946-fb9102befaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def save_model_results(\n",
    "    seed,\n",
    "    acc_test,\n",
    "    f1,\n",
    "    precision,\n",
    "    recall,\n",
    "    acc_test_vote,\n",
    "    f1_vote,\n",
    "    precision_vote,\n",
    "    recall_vote,\n",
    "    test_predictions1,\n",
    "    test_predictions2,\n",
    "    metrics_csv=\"metrics.csv\",\n",
    "    preds_csv=\"predictions.csv\"\n",
    "):\n",
    "\n",
    "    if not os.path.exists(metrics_csv):\n",
    "        df_metrics = pd.DataFrame(columns=[\"seed\",\"acc_test\",\"f1\",\"precision\",\"recall\",\"acc_test_vote\",\"f1_vote\",\"precision_vote\",\"recall_vote\"])\n",
    "    else:\n",
    "        df_metrics = pd.read_csv(metrics_csv)\n",
    "\n",
    "    if seed in df_metrics[\"seed\"].values:\n",
    "        print(f\"[INFO] Metrics for Seed={seed} already exist in {metrics_csv}, skipping save.\")\n",
    "    else:\n",
    "        new_row_df = pd.DataFrame([{\n",
    "            \"seed\": seed,\n",
    "            \"acc_test\": acc_test,\n",
    "            \"f1\": f1,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"acc_test_vote\": acc_test_vote,\n",
    "            \"f1_vote\": f1_vote,\n",
    "            \"precision_vote\": precision_vote,\n",
    "            \"recall_vote\": recall_vote\n",
    "        }])\n",
    "        df_metrics = pd.concat([df_metrics, new_row_df], ignore_index=True)\n",
    "        df_metrics.to_csv(metrics_csv, index=False)\n",
    "        print(f\"[SUCCESS] Saved metrics for Seed={seed} to {metrics_csv}.\")\n",
    "\n",
    "    if not os.path.exists(preds_csv):\n",
    "        df_preds = pd.DataFrame(columns=[\"seed\",\"test_predictions1\",\"test_predictions2\"])\n",
    "    else:\n",
    "        df_preds = pd.read_csv(preds_csv)\n",
    "\n",
    "    if seed in df_preds[\"seed\"].values:\n",
    "        print(f\"[INFO] Prediction list for Seed={seed} already exists in {preds_csv}, skipping save.\")\n",
    "    else:\n",
    "        test_preds1_str = str(test_predictions1)\n",
    "        test_preds2_str = str(test_predictions2)\n",
    "\n",
    "        new_row_preds_df = pd.DataFrame([{\n",
    "            \"seed\": seed,\n",
    "            \"test_predictions1\": test_preds1_str,\n",
    "            \"test_predictions2\": test_preds2_str\n",
    "        }])\n",
    "        df_preds = pd.concat([df_preds, new_row_preds_df], ignore_index=True)\n",
    "        df_preds.to_csv(preds_csv, index=False)\n",
    "        print(f\"[SUCCESS] Saved prediction list for Seed={seed} to {preds_csv}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd266a9-2195-4b31-83a7-c1b01d810b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import jax\n",
    "import optax\n",
    "import math\n",
    "import equinox as eqx\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def get_trainable_params(model):\n",
    "    return eqx.filter(model, eqx.is_inexact_array)\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    num_steps=415, \n",
    "    print_steps=40,  \n",
    "    batch_size=32, \n",
    "    base_lr=3.5e-4, \n",
    "    warmup_steps = 84,\n",
    "    weight_decay=0, \n",
    "    *,\n",
    "    key,\n",
    "    seed,\n",
    "):\n",
    "    global train_predictions1, test_predictions1,test_predictions2,trainable_params\n",
    "    trainable_params = get_trainable_params(model)\n",
    "\n",
    "    warmup_schedule = optax.linear_schedule(\n",
    "        init_value=0.0, \n",
    "        end_value=base_lr, \n",
    "        transition_steps=warmup_steps,  \n",
    "    )\n",
    "    \n",
    "    cosine_schedule = optax.cosine_decay_schedule(\n",
    "        init_value=base_lr,  \n",
    "        decay_steps=num_steps - warmup_steps, \n",
    "        alpha=0.01  \n",
    "    )\n",
    "    \n",
    "    lr_schedule = optax.join_schedules(\n",
    "        schedules=[warmup_schedule, cosine_schedule], \n",
    "        boundaries=[warmup_steps]  \n",
    "    )\n",
    "    \n",
    "    opt = optax.adamw(learning_rate=lr_schedule, weight_decay=weight_decay)\n",
    "    \n",
    "    opt_state = opt.init(eqx.filter(model, eqx.is_inexact_array))\n",
    "\n",
    "    test_accs = [] \n",
    "    test_accs_vote = []\n",
    "    steps = []  \n",
    "    train_accs = [] \n",
    "\n",
    "    dataset_size = X_train.shape[0]\n",
    "    steps_per_epoch = math.ceil(dataset_size / batch_size)\n",
    "    total_epochs = math.ceil(num_steps / steps_per_epoch)\n",
    "\n",
    "    for epoch in range(total_epochs):\n",
    "        print(f\"Epoch: {epoch + 1}\")\n",
    "        trainloopkey, key = jax.random.split(key)\n",
    "\n",
    "        for step, data in zip(\n",
    "            range(steps_per_epoch), train_dataloader.loop(batch_size, key=trainloopkey)\n",
    "        ):\n",
    "            start_time = time.time()\n",
    "    \n",
    "            X, y = data  \n",
    "            key, subkey = jr.split(key)\n",
    "\n",
    "            model, opt_state, loss = train_step(model, X, y, opt, opt_state, key=subkey)\n",
    "            if step == 0 or (step + 1) % print_steps == 0 or step==(steps_per_epoch - 1):\n",
    "                inference_model = eqx.nn.inference_mode(model)\n",
    "                inference_model = eqx.Partial(inference_model,inference=True)\n",
    "                for batch, data in zip(\n",
    "                    range(1), train_dataloader.loop(train_dataloader.size)\n",
    "                ):\n",
    "                    X, y = data\n",
    "                    keys = jax.random.split(jr.PRNGKey(0), X.shape[0])\n",
    "                    output = jax.vmap(inference_model)(X, keys)\n",
    "                    pre_train = output\n",
    "                    train_acc = jnp.mean((output > 0.5) == (y == 1))\n",
    "\n",
    "                for batch, data in zip(\n",
    "                    range(1), test_dataloader.loop(test_dataloader.size)\n",
    "                ):\n",
    "                    X, y = data\n",
    "                    keys = jax.random.split(jr.PRNGKey(0), X.shape[0])\n",
    "                    output = jax.vmap(inference_model)(X, keys)\n",
    "\n",
    "                    test_acc = jnp.mean((output > 0.5) == (y == 1))\n",
    "                if step == steps_per_epoch - 1:\n",
    "                    pre_test = output\n",
    "\n",
    "                elapsed_time = time.time() - start_time\n",
    "                print(f\"Step: {step + 1}, Loss: {loss}, Train Acc: {train_acc}, Test Acc: {test_acc}, Time: {elapsed_time:.4f} seconds\")\n",
    "    \n",
    "                steps.append(step + 1)\n",
    "        audio_segments_train = {}       \n",
    "        \n",
    "        for idx, pred in zip(indices_train[:,0], pre_train):\n",
    "            if idx.size == 1:\n",
    "                idx = int(idx.item())\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected idx size: {idx.size}, idx: {idx}\")\n",
    "            if idx in audio_segments_train:\n",
    "                audio_segments_train[idx].append(pred)\n",
    "            else:\n",
    "                audio_segments_train[idx] = [pred]\n",
    "        \n",
    "        audio_predictions_train = {idx: jnp.mean(jnp.array(preds)) for idx, preds in audio_segments_train.items()}\n",
    "\n",
    "        predictions1 = list(audio_predictions_train.values())\n",
    "        predictions1 = jnp.array(predictions1) \n",
    "\n",
    "\n",
    "        train_predictions1 = [(idx, 1 if pred >= 0.5 else 0) for idx, pred in audio_predictions_train.items()]\n",
    "\n",
    "        audio_segments_test = {}\n",
    "        for idx, pred_val in zip(indices_test[:, 0], pre_test):\n",
    "            if idx.size == 1:\n",
    "                idx = int(idx.item()) \n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected idx size: {idx.size}, idx: {idx}\")\n",
    "            if idx in audio_segments_test:\n",
    "                audio_segments_test[idx].append(pred_val)\n",
    "            else:\n",
    "                audio_segments_test[idx] = [pred_val]\n",
    "        \n",
    "        audio_predictions_test = {idx: jnp.mean(jnp.array(preds)) for idx, preds in audio_segments_test.items()}\n",
    "        audio_predictions_test_vote = {idx: 1 if jnp.sum(jnp.array(preds) > 0.5) > jnp.sum(jnp.array(preds) <= 0.5) else 0 for idx, preds in audio_segments_test.items()}\n",
    "        \n",
    "        values = jnp.array(list(audio_predictions_test.values()))\n",
    "\n",
    "        evaluate_and_plot_roc_pr_curves(labels_train, predictions1, plot_title_prefix=\"Train\")\n",
    "        evaluate_and_plot_roc_pr_curves(labels_test, values, plot_title_prefix=\"Test\")\n",
    "\n",
    "        correct_predictions_test = 0\n",
    "        predict_label=[]\n",
    "        predict_label_vote=[]\n",
    "        for idx, pred in audio_predictions_test.items():\n",
    "            label = labels_test[idx]\n",
    "            predict_label.append((pred>0.5))\n",
    "            if (pred>0.5) == label:\n",
    "                correct_predictions_test += 1\n",
    "                \n",
    "        acc_test = correct_predictions_test / len(audio_predictions_test)\n",
    "                \n",
    "        correct_predictions_test_vote = 0\n",
    "        for idx, pred in audio_predictions_test_vote.items():\n",
    "            label = labels_test[idx]\n",
    "            predict_label_vote.append(pred)\n",
    "            if pred == label:\n",
    "                correct_predictions_test_vote += 1\n",
    "\n",
    "        acc_test_vote = correct_predictions_test_vote / len(audio_predictions_test)\n",
    "\n",
    "        \n",
    "        predict_label_array=np.array(predict_label)\n",
    "        predict_label_vote_array=np.array(predict_label_vote)\n",
    "        labels_test_array=np.array(labels_test)\n",
    "\n",
    "        precision = precision_score(labels_test_array,predict_label_array)\n",
    "        precision_vote=precision_score(labels_test_array,predict_label_vote_array)\n",
    "\n",
    "        recall = recall_score(labels_test_array,predict_label_array)\n",
    "        recall_vote=recall_score(labels_test_array,predict_label_vote_array)\n",
    "\n",
    "        f1 = f1_score(labels_test_array,predict_label_array)\n",
    "        f1_vote=f1_score(labels_test_array,predict_label_vote_array)\n",
    "\n",
    "        print('acc_test:', acc_test)\n",
    "        print('acc_test_vote:', acc_test_vote)\n",
    "        test_accs.append(acc_test)\n",
    "        test_accs_vote.append(acc_test_vote)\n",
    "        if epoch==total_epochs-1:\n",
    "            predictions_list_test = []\n",
    "            for idx, preds in audio_segments_test.items():\n",
    "                mean_pred = jnp.mean(jnp.array(preds))\n",
    "                predictions_list_test.append((idx, mean_pred))\n",
    "            \n",
    "            predictions_list_test.sort(key=lambda x: x[0])\n",
    "            \n",
    "            for idx, mean_pred in predictions_list_test:\n",
    "                print(f\"Audio segment test {idx}: Prediction value {mean_pred}\")\n",
    "            test_predictions1 = [(idx, 1 if pred >= 0.5 else 0) for idx, pred in predictions_list_test]\n",
    "            test_predictions2 = [(idx, 1 if pred == 1 else 0) for idx, pred in audio_predictions_test_vote.items()]\n",
    "            save_model_results(\n",
    "                seed=seed,\n",
    "                acc_test=acc_test,\n",
    "                f1=f1,\n",
    "                precision=precision,\n",
    "                recall=recall,\n",
    "                acc_test_vote=acc_test_vote,\n",
    "                f1_vote=f1_vote,\n",
    "                precision_vote=precision_vote,\n",
    "                recall_vote=recall_vote,\n",
    "                test_predictions1=test_predictions1,\n",
    "                test_predictions2=test_predictions2,\n",
    "                metrics_csv=\"solution/Pitt_log_50seed_acc_h128_v512_0norm_ode250_step60x.csv\",\n",
    "                preds_csv=\"solution/Pitt_log_50seed_predict_h128_v512_0norm_ode250_step60x.csv\"\n",
    "            )\n",
    "        \n",
    "    return acc_test,acc_test_vote,test_accs,test_accs_vote,train_predictions1, test_predictions1,test_predictions2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e487293-cc74-4cf5-8bfa-dee010d05571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f99151-d59b-42df-9ac2-58d26ac9b21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "def train_with_seeds(seeds):\n",
    "    global hyperparameters, all_train_predictions, all_test_predictions_vote, all_test_predictions, acc_seeds\n",
    "    hyperparameters = []\n",
    "    all_train_predictions = []  \n",
    "    all_test_predictions = []  \n",
    "    all_test_predictions_vote = [] \n",
    "    acc_seeds = []  \n",
    "    all_test_accuracies = []\n",
    "    all_test_accuracies_vote = []\n",
    "    for seed in seeds:\n",
    "        key = jax.random.PRNGKey(seed)\n",
    "        modelkey, key = jax.random.split(key)\n",
    "        trainkey, key = jax.random.split(key)\n",
    "\n",
    "        LogNCDE_Depth2 = LogNeuralCDE(\n",
    "            hidden_dim=hidden_dim,\n",
    "            data_dim=data_dim,\n",
    "            label_dim=label_dim,\n",
    "            vf_hidden_dim=vf_hidden_dim,\n",
    "            vf_num_hidden=vf_num_hidden,\n",
    "            ode_solver_stepsize=ode_solver_stepsize,\n",
    "            stepsize=stepsize,\n",
    "            depth=2,\n",
    "            key=modelkey,\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            print(f\"Training with seed: {seed}\")\n",
    "            train_predictions1 = []\n",
    "            test_predictions1 = []\n",
    "            test_predictions2 = []\n",
    "\n",
    "            acc_test, acc_test_vote, test_accs,test_accs_vote,train_predictions1, test_predictions1,test_predictions2 = train_model(LogNCDE_Depth2, key=trainkey, seed=seed)\n",
    "\n",
    "            acc_test_cpu = np.array(acc_test)\n",
    "            acc_test_vote_cpu = np.array(acc_test_vote)\n",
    "            test_accs_cpu = np.array(test_accs)\n",
    "            test_accs_vote_cpu=np.array(test_accs_vote)\n",
    "\n",
    "            test_predictions1_cpu = [np.array(p) for p in test_predictions1]\n",
    "            test_predictions2_cpu = [np.array(p) for p in test_predictions2]\n",
    "\n",
    "            all_train_predictions.append(train_predictions1)\n",
    "            all_test_predictions.append(test_predictions1_cpu)\n",
    "            all_test_predictions_vote.append(test_predictions2_cpu)\n",
    "            all_test_accuracies.append(test_accs_cpu)\n",
    "            all_test_accuracies_vote.append(test_accs_vote_cpu)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error encountered with seed {seed}: {e}\")\n",
    "            placeholder_accs = np.zeros(len(test_accs)) if 'test_accs' in locals() else np.zeros(10)\n",
    "            all_test_accuracies.append(placeholder_accs)\n",
    "            placeholder_accs_vote = np.zeros(len(test_accs_vote)) if 'test_accs_vote' in locals() else np.zeros(10)\n",
    "            all_test_accuracies_vote.append(placeholder_accs_vote)\n",
    "            continue\n",
    "\n",
    "        finally:\n",
    "            del LogNCDE_Depth2\n",
    "            del key\n",
    "            del modelkey\n",
    "            del trainkey\n",
    "            jax.device_put(None)\n",
    "            gc.collect()\n",
    "            jax.clear_caches()\n",
    "\n",
    "    print(\"test_predictions:\", all_test_predictions)\n",
    "    print(\"test_predictions_vote:\", all_test_predictions_vote)\n",
    "\n",
    "    vote_and_evaluate()\n",
    "\n",
    "    epochs = range(1, len(all_test_accuracies[0]) + 1)\n",
    "    df_test = pd.DataFrame(all_test_accuracies, columns=epochs, index=[f\"Seed_{s}\" for s in seeds])\n",
    "    df_test_vote = pd.DataFrame(all_test_accuracies_vote, columns=epochs, index=[f\"Seed_{s}\" for s in seeds])\n",
    "    print(\"\\nTest Accuracies per Epoch per Seed:\")\n",
    "    print(df_test)\n",
    "    print(\"\\nTest Accuracies vote per Epoch per Seed:\")\n",
    "    print(df_test_vote)\n",
    "    return df_test\n",
    "\n",
    "\n",
    "def vote_and_evaluate():\n",
    "    train_votes = aggregate_votes(all_train_predictions)\n",
    "    accuracy=evaluate_accuracy(train_votes, labels_train, \"Train\")\n",
    "\n",
    "    test_votes = aggregate_votes(all_test_predictions)\n",
    "    accuracy_test=evaluate_accuracy(test_votes, labels_test, \"Test\")\n",
    "    test_votes=jnp.array(list(test_votes.values()))\n",
    "\n",
    "    test_vote_votes = aggregate_votes(all_test_predictions_vote)\n",
    "    accuracy_vote_test=evaluate_accuracy(test_vote_votes, labels_test, \"Test_vote\")\n",
    "    test_vote_votes=jnp.array(list(test_vote_votes.values()))\n",
    "    \n",
    "    precision = precision_score(labels_test,test_vote_votes)\n",
    "    recall = recall_score(labels_test,test_vote_votes)\n",
    "    f1 = f1_score(labels_test,test_vote_votes)\n",
    "    print('testvotes:',test_votes)\n",
    "    print('test_vote_votes:',test_vote_votes)\n",
    "\n",
    "def aggregate_votes(predictions_list_all_seeds):\n",
    "    aggregated_predictions = {}\n",
    "    for idx in range(len(predictions_list_all_seeds[0])):\n",
    "        preds = [predictions_list_all_seeds[seed_idx][idx][1] for seed_idx in range(len(predictions_list_all_seeds))]\n",
    "        vote_result = 1 if sum(pred == 1 for pred in preds) > len(preds) / 2 else 0\n",
    "        aggregated_predictions[idx] = vote_result\n",
    "    return aggregated_predictions\n",
    "\n",
    "\n",
    "def evaluate_accuracy(aggregated_predictions, labels, dataset_name):\n",
    "    correct_predictions = 0\n",
    "    for idx, pred in aggregated_predictions.items():\n",
    "        label = labels[idx]\n",
    "        if pred == label:\n",
    "            correct_predictions += 1\n",
    "    accuracy = correct_predictions / len(aggregated_predictions)\n",
    "    print(f'{dataset_name} Accuracy (after voting): {accuracy:.2f}')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95411101-f80f-4f1a-afe8-bf6bf24ed6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 128\n",
    "data_dim = 33\n",
    "label_dim = 1\n",
    "vf_hidden_dim = 512\n",
    "vf_num_hidden = 3\n",
    "ode_solver_stepsize = 1 / 250\n",
    "stepsize = 60\n",
    "num_seeds = 5\n",
    "seeds = np.random.randint(0, 10000, size=num_seeds).tolist()\n",
    "test_seeds=[1001,1002,1003,1004,1005]\n",
    "train_with_seeds(test_seeds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
