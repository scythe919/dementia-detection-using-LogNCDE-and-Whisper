{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c4c81e-97e4-46c1-a5f9-033a3bb4155c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "from transformers import WhisperProcessor, WhisperModel\n",
    "\n",
    "segment_length1 = 480000  \n",
    "overlap_rate1 = 0.3      \n",
    "sampling_rate = 16000    \n",
    "\n",
    "audio_path_full_enhance_cc_train = \"/home/sichengyu/Downloads/dementiabank/Pitt_new/norm/train_test_split/train/train_split/folds/fold_5/train/cc_enhence\"\n",
    "audio_path_full_enhance_cd_train = \"/home/sichengyu/Downloads/dementiabank/Pitt_new/norm/train_test_split/train/train_split/folds/fold_5/train/cd_enhence\"\n",
    "audio_path_full_enhance_cc_test = \"/home/sichengyu/Downloads/dementiabank/Pitt_new/norm/train_test_split/train/train_split/folds/fold_5/val/cc_enhence\"\n",
    "audio_path_full_enhance_cd_test = \"/home/sichengyu/Downloads/dementiabank/Pitt_new/norm/train_test_split/train/train_split/folds/fold_5/val/cd_enhence\"\n",
    "\n",
    "feature_dic = {\n",
    "    'list_full_cc_train': [],\n",
    "    'list_full_cd_train': [],\n",
    "    'list_norm_cc_train': [],\n",
    "    'list_norm_cd_train': [],\n",
    "    'list_full_cc_test': [],\n",
    "    'list_full_cd_test': [],\n",
    "    'list_norm_cc_test': [],\n",
    "    'list_norm_cd_test': []\n",
    "}\n",
    "\n",
    "model_name = \"openai/whisper-small\"\n",
    "processor = WhisperProcessor.from_pretrained(model_name)\n",
    "model = WhisperModel.from_pretrained(model_name, output_hidden_states=True)\n",
    "model.eval()\n",
    "\n",
    "def get_segment_features(audio_file, audio_file_index, segment_length=segment_length1, sampling_rate=16000, overlap_rate=overlap_rate1):\n",
    "    \"\"\"\n",
    "    Extract features from a single audio file.\n",
    "\n",
    "    Parameters:\n",
    "        audio_file (str): Audio file path.\n",
    "        audio_file_index (int): Index of the audio file.\n",
    "        segment_length (int): Length of each segment.\n",
    "        sampling_rate (int): Sampling rate.\n",
    "        overlap_rate (float): Overlap rate.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (list of segment features, list of segment indices)\n",
    "    \"\"\"\n",
    "\n",
    "    y, sr = librosa.load(audio_file, sr=sampling_rate)\n",
    "    step_size = int(segment_length * (1 - overlap_rate))\n",
    "    total_segments = int(len(y) / step_size) + (1 if len(y) % step_size > overlap_rate * segment_length else 0)\n",
    "\n",
    "    segment_features_list = []\n",
    "    segment_indices = []\n",
    "\n",
    "    for i in range(total_segments):\n",
    "        start = i * step_size\n",
    "        end = start + segment_length\n",
    "        segment = y[start:end]\n",
    "\n",
    "        # If the last segment's length is insufficient, pad with the ending portion of the previous segment\n",
    "        if len(segment) < segment_length and i > 0:  # Ensure it's not the first segment\n",
    "            padding_needed = segment_length - len(segment)\n",
    "            # Get the ending part of the previous segment for padding\n",
    "            previous_segment_end = start + segment_length - step_size  # End of the previous segment\n",
    "            padding_start = max(0, previous_segment_end - padding_needed)  # Ensure not to exceed the start of the audio file\n",
    "            padding_values = y[padding_start:previous_segment_end]\n",
    "            segment = np.concatenate((segment, padding_values))\n",
    "\n",
    "        # Preprocess the current segment and extract features\n",
    "        inputs = processor(segment, sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "        input_features = inputs.input_features  # Whisper uses input_features\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Only call the encoder part\n",
    "            encoder_outputs = model.encoder(input_features=input_features, output_hidden_states=True)\n",
    "            hidden_states = encoder_outputs.hidden_states  # Get all hidden layers\n",
    "\n",
    "        num_layers = len(hidden_states)\n",
    "\n",
    "        # Select the layer to extract (e.g., the 12th layer)\n",
    "        desired_layer = 12 \n",
    "        if num_layers > desired_layer:\n",
    "            layer_features = hidden_states[desired_layer].squeeze(0)  # Remove the first dimension (batch_size)\n",
    "            segment_features_list.append(layer_features)\n",
    "            segment_indices.append((audio_file_index, i))\n",
    "        else:\n",
    "            print(f\"Encoder has only {num_layers} layers, cannot extract features from layer {desired_layer + 1}.\")\n",
    "            continue\n",
    "\n",
    "    return segment_features_list, segment_indices\n",
    "\n",
    "def get_features_from_directory(audio_path, segment_length=segment_length1, sampling_rate=16000, overlap_rate=0.3):\n",
    "    \"\"\"\n",
    "    Extract features from all audio files in the specified directory.\n",
    "\n",
    "    Parameters:\n",
    "        audio_path (str): Audio file directory path.\n",
    "        segment_length (int): Length of each segment.\n",
    "        sampling_rate (int): Sampling rate.\n",
    "        overlap_rate (float): Overlap rate.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (list of all segment features, list of all segment indices)\n",
    "    \"\"\"\n",
    "    audio_files = [os.path.join(audio_path, file) for file in os.listdir(audio_path) if file.endswith('.wav')]\n",
    "\n",
    "    features_list = []\n",
    "    segments_indices = []\n",
    "\n",
    "    for audio_file_index, audio_file in enumerate(audio_files):\n",
    "        print(f\"Processing file: {audio_file} (Index: {audio_file_index})\")\n",
    "        segment_features, segment_indices = get_segment_features(\n",
    "            audio_file,\n",
    "            audio_file_index,\n",
    "            segment_length=segment_length,\n",
    "            sampling_rate=sampling_rate,\n",
    "            overlap_rate=overlap_rate\n",
    "        )\n",
    "        features_list.extend(segment_features)\n",
    "        segments_indices.extend(segment_indices)\n",
    "\n",
    "    return features_list, segments_indices\n",
    "\n",
    "def normalize_features(features_list):\n",
    "    \"\"\"\n",
    "    Normalize the features.\n",
    "\n",
    "    Parameters:\n",
    "        features_list (list of torch.Tensor): List of features.\n",
    "\n",
    "    Returns:\n",
    "        list of torch.Tensor: List of normalized features.\n",
    "    \"\"\"\n",
    "    normalized_features = []\n",
    "    for feature in features_list:\n",
    "        feature_np = feature.numpy()\n",
    "        feature_norm = (feature_np - feature_np.min()) / (feature_np.max() - feature_np.min() + 1e-8)\n",
    "        feature_norm_tensor = torch.from_numpy(feature_norm)\n",
    "        normalized_features.append(feature_norm_tensor)\n",
    "    return normalized_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892c76e2-b7b2-47d7-91aa-aae9916bdec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dic['list_full_cc_train'], indices_cc_train = get_features_from_directory(\n",
    "    audio_path_full_enhance_cc_train,\n",
    "    segment_length=segment_length1,\n",
    "    sampling_rate=sampling_rate,\n",
    "    overlap_rate=overlap_rate1\n",
    ")\n",
    "\n",
    "feature_dic['list_full_cd_train'], indices_cd_train = get_features_from_directory(\n",
    "    audio_path_full_enhance_cd_train,\n",
    "    segment_length=segment_length1,\n",
    "    sampling_rate=sampling_rate,\n",
    "    overlap_rate=overlap_rate1\n",
    ")\n",
    "\n",
    "feature_dic['list_full_cc_test'], indices_cc_test = get_features_from_directory(\n",
    "    audio_path_full_enhance_cc_test,\n",
    "    segment_length=segment_length1,\n",
    "    sampling_rate=sampling_rate,\n",
    "    overlap_rate=overlap_rate1\n",
    ")\n",
    "\n",
    "feature_dic['list_full_cd_test'], indices_cd_test = get_features_from_directory(\n",
    "    audio_path_full_enhance_cd_test,\n",
    "    segment_length=segment_length1,\n",
    "    sampling_rate=sampling_rate,\n",
    "    overlap_rate=overlap_rate1\n",
    ")\n",
    "\n",
    "feature_dic['list_norm_cc_train'] = normalize_features(feature_dic['list_full_cc_train'])\n",
    "feature_dic['list_norm_cd_train'] = normalize_features(feature_dic['list_full_cd_train'])\n",
    "feature_dic['list_norm_cc_test'] = normalize_features(feature_dic['list_full_cc_test'])\n",
    "feature_dic['list_norm_cd_test'] = normalize_features(feature_dic['list_full_cd_test'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547a36c5-ec8e-444c-869e-b231faa6d46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_cc_train = np.array(indices_cc_train)\n",
    "indices_cd_train = np.array(indices_cd_train)\n",
    "\n",
    "max_index_cc_train = indices_cc_train[:, 0].max()\n",
    "\n",
    "indices_cd_train_adjusted = indices_cd_train.copy()\n",
    "indices_cd_train_adjusted[:, 0] += max_index_cc_train + 1\n",
    "\n",
    "indices_train = np.vstack((indices_cc_train, indices_cd_train_adjusted))\n",
    "indices_train = jnp.array(indices_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c89045a-3a83-4814-9b59-3c304c0c3b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_cc_test = np.array(indices_cc_test)\n",
    "indices_cd_test = np.array(indices_cd_test)\n",
    "\n",
    "max_index_cc_test = indices_cc_test[:, 0].max()\n",
    "\n",
    "indices_cd_test_adjusted = indices_cd_test.copy()\n",
    "indices_cd_test_adjusted[:, 0] += max_index_cc_test + 1\n",
    "\n",
    "indices_test = np.vstack((indices_cc_test, indices_cd_test_adjusted))\n",
    "indices_test = jnp.array(indices_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef273c3f-3622-4d2d-bf6a-b609b6d26533",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_cc = torch.stack(feature_dic['list_full_cc_train'], dim=0)\n",
    "features_cd = torch.stack(feature_dic['list_full_cd_train'], dim=0)\n",
    "features_cc_test = torch.stack(feature_dic['list_full_cc_test'], dim=0)\n",
    "features_cd_test = torch.stack(feature_dic['list_full_cd_test'], dim=0)\n",
    "\n",
    "features1 = torch.cat([features_cc, features_cd], dim=0)\n",
    "features2= torch.cat([features_cc_test, features_cd_test], dim=0)\n",
    "\n",
    "labels_cc = torch.zeros(len(feature_dic['list_full_cc_train']))\n",
    "labels_cd = torch.ones(len(feature_dic['list_full_cd_train']))\n",
    "labels_cc_test = torch.zeros(len(feature_dic['list_full_cc_test']))\n",
    "labels_cd_test = torch.ones(len(feature_dic['list_full_cd_test']))\n",
    "labels1 = torch.cat([labels_cc, labels_cd], dim=0)\n",
    "labels2 = torch.cat([labels_cc_test, labels_cd_test], dim=0)\n",
    "\n",
    "features1_s = features1.permute(0, 2, 1)\n",
    "features2_s = features2.permute(0, 2, 1)\n",
    "print('features1.shape:',features1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6499e3-c7d7-4e83-aae8-522b11e50918",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/home/sichengyu/text/NCDE'\n",
    "feature_folder = 'feature_tensor/Pittnew/CV5'\n",
    "save_path = os.path.join(base_path, feature_folder)\n",
    "\n",
    "file_name = 'feature_Pitt_0.3_train1_whisper_30_new.pt'\n",
    "file_name2 = 'feature_Pitt_0.3_val_whisper_30_new.pt'\n",
    "file_name3 = 'labels1_Pitt_0.3_train1_whisper_30_new.pt'\n",
    "file_name4 = 'labels2_Pitt_0.3_val_whisper_30_new.pt'\n",
    "file_name5 = 'indices_train1_Pitt_0.3_whisper_30_new.pt'\n",
    "file_name6 = 'indices_val_Pitt_0.3_whisper_30_new.pt'\n",
    "\n",
    "full_path = os.path.join(save_path, file_name)\n",
    "full_path2 = os.path.join(save_path, file_name2)\n",
    "full_path3 = os.path.join(save_path, file_name3)\n",
    "full_path4 = os.path.join(save_path, file_name4)\n",
    "full_path5 = os.path.join(save_path, file_name5)\n",
    "full_path6 = os.path.join(save_path, file_name6)\n",
    "\n",
    "torch.save(features1, full_path)\n",
    "torch.save(features2, full_path2)\n",
    "torch.save(labels1,full_path3)\n",
    "torch.save(labels2, full_path4)\n",
    "torch.save(indices_train, full_path5)\n",
    "torch.save(indices_test, full_path6)\n",
    "\n",
    "print(f\"Tensor saved to {full_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c411825-08cc-4b44-82d1-1817574f8dac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
